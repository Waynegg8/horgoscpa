name: éƒ¨è½æ ¼èˆ‡å½±ç‰‡è‡ªå‹•åŒ–è™•ç†

on:
  workflow_dispatch:
    inputs:
      process_word:
        description: 'è™•ç†Wordæ–‡æª”'
        required: true
        default: 'true'
        type: boolean
      update_json:
        description: 'æ›´æ–°JSONæ–‡ä»¶'
        required: true
        default: 'true'
        type: boolean
      handle_deletion:
        description: 'è™•ç†æ–‡ç« åˆªé™¤'
        required: true
        default: 'true'
        type: boolean
      update_videos:
        description: 'æ›´æ–°å½±ç‰‡æ•¸æ“š'
        required: true
        default: 'true'
        type: boolean
      force_scan:
        description: 'å¼·åˆ¶æƒææ‰€æœ‰HTMLæ–‡ç« '
        required: true
        default: 'false'
        type: boolean
      update_sitemap:
        description: 'æ›´æ–°ç¶²ç«™Sitemap'
        required: true
        default: 'true'
        type: boolean
      publish_scheduled:
        description: 'ç™¼å¸ƒæ’ç¨‹æ–‡ç« '
        required: true
        default: 'true'
        type: boolean
      update_dictionary:
        description: 'æ›´æ–°ç¿»è­¯å­—å…¸'
        required: true
        default: 'true'
        type: boolean
      deploy_ai_chat:
        description: 'éƒ¨ç½²AIå®¢æœåŠŸèƒ½'
        required: true
        default: 'true'
        type: boolean
  
  push:
    paths:
      - '**/*'
  
  delete:
    paths:
      - 'blog/**/*.html'
      - 'services/**/*.html'
      - '*.html'
  
  schedule:
    - cron: '0 1 * * *'
    
  repository_dispatch:
    types: [delete-article, upload-article, scan-articles, update-video, update-sitemap, publish-scheduled, update-dictionary, deploy-ai-chat]

jobs:
  content-automation:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: write
    
    steps:
      - name: æª¢å‡ºä»£ç¢¼
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: è¨­ç½® Git é…ç½®
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
      
      - name: è¨­ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: å®‰è£ä¾è³´å¥—ä»¶
        run: |
          python -m pip install --upgrade pip
          pip install loguru python-docx beautifulsoup4 requests pathlib
      
      - name: å‰µå»ºå¿…è¦çš„ç›®éŒ„
        run: |
          mkdir -p blog
          mkdir -p assets/data
          mkdir -p assets/images/blog
          mkdir -p assets/css
          mkdir -p assets/js
          mkdir -p word-docs/processed
          mkdir -p video
          mkdir -p .github/scripts
          mkdir -p logs
          mkdir -p scripts

      - name: é‡å»ºåŸæœ‰æ¨¡çµ„çµæ§‹
        run: |
          # æª¢æŸ¥ä¸¦å‰µå»ºåŸæœ‰çš„æ¨¡çµ„æ–‡ä»¶
          echo "é‡å»ºåŸæœ‰çš„å®Œæ•´æ¨¡çµ„çµæ§‹..."
          
          # å¦‚æœåŸæœ‰æ–‡ä»¶å­˜åœ¨ï¼Œä½¿ç”¨åŸæœ‰æ–‡ä»¶ï¼›å¦å‰‡é‡å»º
          if [ ! -f "word_processor.py" ] && [ ! -f "scripts/word_processor.py" ]; then
            cat > word_processor.py << 'WORD_PROCESSOR_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from loguru import logger
import docx
from docx.opc.exceptions import PackageNotFoundError

from utils import parse_filename, read_json, write_json
from translator import get_translator

class ImprovedWordProcessor:
    """æ”¹é€²ç‰ˆ Word æ–‡æª”è™•ç†é¡"""
    
    def __init__(self, word_dir="word-docs", processed_dir=None, translation_dict_file=None, api_key=None):
        self.word_dir = Path(word_dir)
        self.processed_dir = Path(processed_dir or os.path.join(word_dir, "processed"))
        self.translation_dict_file = Path(translation_dict_file or "assets/data/translation_dict.json")
        self.processed_files_file = Path("assets/data/processed_files.json")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.word_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.translation_dict_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç¿»è­¯å™¨
        self.translator = get_translator(self.translation_dict_file, api_key)
        
        # è¼‰å…¥å·²è™•ç†æ–‡ä»¶è¨˜éŒ„
        self.processed_files = read_json(self.processed_files_file, default={"files": []})
        if "files" not in self.processed_files:
            self.processed_files["files"] = []
        
        # è¼‰å…¥é—œéµè©æ˜ å°„è¡¨
        self.keyword_mappings = self._load_keyword_mappings()
    
    def _load_keyword_mappings(self):
        """è¼‰å…¥ä¸­è‹±æ–‡é—œéµè©æ˜ å°„è¡¨"""
        mappings_file = Path("assets/data/keyword_mappings.json")
        
        # é è¨­çš„é—œéµè©æ˜ å°„
        default_mappings = {
            # ç¨…å‹™ç›¸é—œ
            "ç¨…å‹™": "tax", "ç¨…æ³•": "tax-law", "ç¨…å‹™è¦åŠƒ": "tax-planning",
            "ç¯€ç¨…": "tax-saving", "é¿ç¨…": "tax-avoidance", "ç¨…å‹™ç”³å ±": "tax-filing",
            "éºç”¢ç¨…": "estate-tax", "è´ˆèˆ‡ç¨…": "gift-tax", "æ‰€å¾—ç¨…": "income-tax",
            "ç‡Ÿæ¥­ç¨…": "business-tax", "æˆ¿å±‹ç¨…": "house-tax", "åœ°åƒ¹ç¨…": "land-value-tax",
            "å°èŠ±ç¨…": "stamp-tax",
            
            # ä¿éšªç›¸é—œ
            "ä¿éšª": "insurance", "äººå£½ä¿éšª": "life-insurance", "ä¿éšªçµ¦ä»˜": "insurance-benefit",
            "ä¿éšªè¦åŠƒ": "insurance-planning", "ä¿å–®": "insurance-policy", "å—ç›Šäºº": "beneficiary",
            "è¦ä¿äºº": "policyholder", "è¢«ä¿éšªäºº": "insured",
            
            # æœƒè¨ˆç›¸é—œ
            "æœƒè¨ˆ": "accounting", "å¸³å‹™": "bookkeeping", "è²¡å‹™": "finance",
            "å¯©è¨ˆ": "audit", "è²¡å ±": "financial-report", "æˆæœ¬": "cost",
            "è²»ç”¨": "expense", "æ”¶å…¥": "income", "è³‡ç”¢": "asset", "è² å‚µ": "liability",
            
            # ä¼æ¥­ç›¸é—œ
            "ä¼æ¥­": "business", "å…¬å¸": "company", "å·¥å•†ç™»è¨˜": "business-registration",
            "ç‡Ÿé‹": "operation", "æŠ•è³‡": "investment", "ç†è²¡": "financial-planning",
        }
        
        # å¦‚æœæ˜ å°„æ–‡ä»¶å­˜åœ¨ï¼Œè¼‰å…¥ä¸¦åˆä½µ
        if mappings_file.exists():
            try:
                custom_mappings = read_json(mappings_file)
                default_mappings.update(custom_mappings)
            except Exception as e:
                logger.warning(f"è¼‰å…¥è‡ªå®šç¾©é—œéµè©æ˜ å°„å¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­æ˜ å°„")
        else:
            # ä¿å­˜é è¨­æ˜ å°„åˆ°æ–‡ä»¶
            write_json(mappings_file, default_mappings)
        
        return default_mappings
    
    def scan_documents(self, process_all=False, process_date=None, current_date=None):
        """æƒæ Word æ–‡æª”ç›®éŒ„"""
        if current_date is None:
            current_date = datetime.now().date()
        
        documents = []
        skipped_current_future = []
        
        word_extensions = [".docx", ".doc"]
        
        logger.info(f"é–‹å§‹æƒææ–‡ä»¶å¤¾: {self.word_dir}, ç•¶å‰æ—¥æœŸ: {current_date}")
        
        for file in self.word_dir.glob("*"):
            if file.is_dir():
                continue
            
            if file.suffix.lower() not in word_extensions:
                continue
            
            if str(file) in self.processed_files["files"]:
                logger.debug(f"è·³éå·²è™•ç†æ–‡ä»¶: {file}")
                continue
            
            file_info = parse_filename(file.name)
            if not file_info:
                logger.warning(f"ç„¡æ³•è§£ææª”å: {file.name}")
                continue
            
            try:
                file_date = datetime.strptime(file_info["date"], "%Y-%m-%d").date()
            except ValueError:
                logger.warning(f"æª”åä¸­çš„æ—¥æœŸæ ¼å¼ç„¡æ•ˆ: {file_info['date']}")
                continue
            
            if process_all:
                documents.append(file)
            elif process_date:
                if file_date == process_date:
                    documents.append(file)
            else:
                if file_date <= current_date:
                    documents.append(file)
                else:
                    skipped_current_future.append(str(file))
        
        if skipped_current_future:
            logger.warning(f"è·³éäº†{len(skipped_current_future)}å€‹æœªä¾†æ—¥æœŸçš„æ–‡ä»¶")
        
        logger.info(f"æ‰¾åˆ°{len(documents)}å€‹éœ€è¦è™•ç†çš„æ–‡ä»¶")
        return documents
    
    def extract_content(self, doc_path):
        """æå– Word æ–‡æª”å…§å®¹"""
        doc_path = Path(doc_path)
        
        file_info = parse_filename(doc_path.name)
        if not file_info:
            raise ValueError(f"ç„¡æ³•è§£ææª”å: {doc_path.name}")
        
        try:
            doc = docx.Document(doc_path)
        except PackageNotFoundError:
            raise ValueError(f"ç„¡æ³•è®€å– Word æ–‡æª”: {doc_path}")
        
        title = doc.paragraphs[0].text.strip() if doc.paragraphs else file_info["title"]
        
        content = []
        for i, para in enumerate(doc.paragraphs):
            if not para.text.strip():
                continue
            
            if i == 0 and para.text.strip() == title:
                continue
            
            content.append(para.text)
        
        summary = ""
        if content:
            summary = content[0]
            if len(content) > 1:
                summary += " " + content[1]
            summary = summary[:200] + "..." if len(summary) > 200 else summary
        
        result = {
            "filename": doc_path.name,
            "original_filename": doc_path.name,
            "file_info": file_info,
            "title": title,
            "content": content,
            "summary": summary,
            "date": file_info["date"],
            "source_path": str(doc_path)
        }
        
        if file_info.get("is_series", False):
            result["is_series"] = True
            result["series_name"] = file_info["series_name"]
            result["episode"] = file_info["episode"]
        else:
            result["is_series"] = False
        
        return result
    
    def generate_meaningful_url(self, doc_info):
        """ç”Ÿæˆç°¡æ½”æœ‰æ„ç¾©çš„ SEO å‹å–„ URL"""
        # æå–æ—¥æœŸ - å¾æª”åè§£æçš„æ—¥æœŸ
        date = doc_info["date"]
        
        # åˆå§‹åŒ– URL çµ„ä»¶
        url_components = [date]
        
        # è™•ç†ç³»åˆ—æ–‡ç« 
        if doc_info.get("is_series", False):
            series_name = doc_info.get("series_name", "")
            if series_name:
                # æå–ç³»åˆ—åç¨±çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆåªå–æœ€é‡è¦çš„ä¸€å€‹è©ï¼‰
                series_core = self._extract_core_concept(series_name)
                if series_core:
                    url_components.append(series_core)
            
            # æ·»åŠ é›†æ•¸
            url_components.append(f"ep{doc_info['episode']}")
        else:
            # éç³»åˆ—æ–‡ç« ï¼Œæå–æ¨™é¡Œçš„æ ¸å¿ƒæ„æ¶µ
            title = doc_info.get("title", "")
            if title:
                # åªæå–1-2å€‹æœ€æ ¸å¿ƒçš„æ¦‚å¿µ
                core_concepts = self._extract_core_concepts(title, max_concepts=2)
                if core_concepts:
                    url_components.extend(core_concepts)
        
        # çµ„åˆURL
        url = "-".join(url_components)
        
        # æœ€çµ‚æ¸…ç†ï¼Œç¢ºä¿ç°¡æ½”
        url = self._clean_and_simplify_url(url)
        
        return url
    
    def prepare_document(self, doc_path):
        """æº–å‚™æ–‡æª”è™•ç†ï¼ˆçµ±ä¸€æ¥å£ï¼‰"""
        try:
            doc_info = self.extract_content(doc_path)
            doc_info["url"] = self.generate_meaningful_url(doc_info)
            doc_info["prepared"] = True
            return doc_info
        except Exception as e:
            return {"prepared": False, "error": str(e)}
    
    def finalize_document_processing(self, doc_info, success=False):
        """å®Œæˆæ–‡æª”è™•ç†"""
        if success:
            # ç§»å‹•æ–‡ä»¶åˆ°å·²è™•ç†ç›®éŒ„
            source_path = Path(doc_info["source_path"])
            if source_path.exists():
                processed_path = self.processed_dir / source_path.name
                shutil.move(str(source_path), str(processed_path))
                
                # è¨˜éŒ„å·²è™•ç†æ–‡ä»¶
                if str(source_path) not in self.processed_files["files"]:
                    self.processed_files["files"].append(str(source_path))
                    write_json(self.processed_files_file, self.processed_files)
                
                doc_info["processed"] = True
                doc_info["processed_path"] = str(processed_path)
            else:
                doc_info["processed"] = False
                doc_info["error"] = "æºæ–‡ä»¶ä¸å­˜åœ¨"
        else:
            doc_info["processed"] = False
        
        return doc_info
    
    def _extract_meaningful_keywords(self, text):
        """å¾æ–‡æœ¬ä¸­æå–æœ‰æ„ç¾©çš„é—œéµè©"""
        keywords = []
        
        # 1. å…ˆåŒ¹é…å·²çŸ¥çš„å°ˆæ¥­è©å½™æ˜ å°„
        for zh_term, en_term in self.keyword_mappings.items():
            if zh_term in text:
                keywords.append((zh_term, en_term))
        
        # 2. å¦‚æœé—œéµè©ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†è©
        if len(keywords) < 3:
            extracted_terms = self._intelligent_term_extraction(text)
            for term in extracted_terms:
                if term not in [k[0] for k in keywords]:
                    # å°æœªæ˜ å°„çš„è©å½™é€²è¡Œç¿»è­¯
                    translated = self.translator.translate(term, clean_url=True)
                    keywords.append((term, translated))
        
        # 3. å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_keywords = []
        seen_en = set()
        for zh, en in keywords:
            if en not in seen_en and len(unique_keywords) < 5:
                unique_keywords.append((zh, en))
                seen_en.add(en)
        
        return unique_keywords
    
    def _intelligent_term_extraction(self, text):
        """æ™ºèƒ½è©å½™æå–"""
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
        clean_text = re.sub(r'[^\w\s]', '', text)
        
        # è²¡ç¨…å°ˆæ¥­è©å½™ç‰¹å¾µæ¨¡å¼
        professional_patterns = [
            r'\w*ç¨…\w*',     # åŒ…å«"ç¨…"çš„è©
            r'\w*éšª\w*',     # åŒ…å«"éšª"çš„è©
            r'\w*å‹™\w*',     # åŒ…å«"å‹™"çš„è©
            r'\w*è¨ˆ\w*',     # åŒ…å«"è¨ˆ"çš„è©
            r'\w*è²¡\w*',     # åŒ…å«"è²¡"çš„è©
            r'\w*æ³•\w*',     # åŒ…å«"æ³•"çš„è©
            r'\w*è¦\w*',     # åŒ…å«"è¦"çš„è©
            r'\w*åŠƒ\w*',     # åŒ…å«"åŠƒ"çš„è©
        ]
        
        extracted_terms = []
        
        # ä½¿ç”¨æ¨¡å¼åŒ¹é…æå–å°ˆæ¥­è©å½™
        for pattern in professional_patterns:
            matches = re.findall(pattern, clean_text)
            for match in matches:
                if len(match) >= 2 and len(match) <= 6:  # åˆç†çš„è©å½™é•·åº¦
                    extracted_terms.append(match)
        
        # å»é‡ä¸¦æ’åºï¼ˆæŒ‰é•·åº¦é™åºï¼Œå„ªå…ˆé¸æ“‡è¼ƒé•·çš„è©å½™ï¼‰
        unique_terms = list(set(extracted_terms))
        unique_terms.sort(key=len, reverse=True)
        
        return unique_terms[:5]
    
    def _extract_core_concept(self, text):
        """æå–å–®ä¸€æ ¸å¿ƒæ¦‚å¿µ"""
        # æ ¸å¿ƒæ¦‚å¿µå„ªå…ˆç´šåˆ—è¡¨ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
        core_concepts = {
            # ç¨…å‹™æ ¸å¿ƒæ¦‚å¿µ
            "æˆ¿åœ°åˆä¸€": "real-estate-tax",
            "éºç”¢ç¨…": "estate-tax",
            "è´ˆèˆ‡ç¨…": "gift-tax", 
            "æ‰€å¾—ç¨…": "income-tax",
            "ç‡Ÿæ¥­ç¨…": "business-tax",
            "ç¶œæ‰€ç¨…": "income-tax",
            "ç¨…å‹™": "tax",
            
            # ä¿éšªæ ¸å¿ƒæ¦‚å¿µ
            "äººå£½ä¿éšª": "life-insurance",
            "ä¿éšªçµ¦ä»˜": "insurance-benefit",
            "ä¿éšª": "insurance",
            
            # æœƒè¨ˆæ ¸å¿ƒæ¦‚å¿µ
            "è²¡å‹™": "finance",
            "æœƒè¨ˆ": "accounting",
            "å¯©è¨ˆ": "audit",
            
            # æˆ¿åœ°ç”¢æ ¸å¿ƒæ¦‚å¿µ
            "æˆ¿åœ°ç”¢": "real-estate",
            "ä¸å‹•ç”¢": "real-estate",
            "æˆ¿å±‹": "property",
            
            # å…¬å¸ç›¸é—œ
            "å…¬å¸": "company",
            "ä¼æ¥­": "business"
        }
        
        # æŒ‰é•·åº¦æ’åºï¼Œå„ªå…ˆåŒ¹é…è¼ƒé•·çš„è©å½™
        for concept in sorted(core_concepts.keys(), key=len, reverse=True):
            if concept in text:
                return core_concepts[concept]
        
        # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°æ ¸å¿ƒæ¦‚å¿µï¼Œè¿”å›é€šç”¨ç¿»è­¯
        return self.translator.translate(text[:6], clean_url=True)  # åªå–å‰6å€‹å­—ç¬¦
    
    def _extract_core_concepts(self, text, max_concepts=2):
        """æå–å¤šå€‹æ ¸å¿ƒæ¦‚å¿µï¼Œä½†é™åˆ¶æ•¸é‡"""
        concepts = []
        
        # é«˜å„ªå…ˆç´šæ¦‚å¿µï¼ˆé€™äº›æ¦‚å¿µæ›´é‡è¦ï¼Œå„ªå…ˆæå–ï¼‰
        high_priority = {
            "æˆ¿åœ°åˆä¸€": "real-estate-tax",
            "éºç”¢ç¨…": "estate-tax", 
            "è´ˆèˆ‡ç¨…": "gift-tax",
            "æ‰€å¾—ç¨…": "income-tax",
            "ç‡Ÿæ¥­ç¨…": "business-tax",
            "äººå£½ä¿éšª": "life-insurance",
            "ä¿éšªçµ¦ä»˜": "insurance-benefit",
        }
        
        # ä¸­å„ªå…ˆç´šæ¦‚å¿µ
        medium_priority = {
            "ç¨…å‹™": "tax",
            "ä¿éšª": "insurance", 
            "æœƒè¨ˆ": "accounting",
            "è²¡å‹™": "finance",
            "æˆ¿åœ°ç”¢": "real-estate",
            "å…¬å¸": "company",
            "ç”³å ±": "filing",
            "è¦åŠƒ": "planning"
        }
        
        # å…ˆæª¢æŸ¥é«˜å„ªå…ˆç´šæ¦‚å¿µ
        for concept, translation in high_priority.items():
            if concept in text and len(concepts) < max_concepts:
                concepts.append(translation)
                text = text.replace(concept, "")  # ç§»é™¤å·²åŒ¹é…çš„éƒ¨åˆ†
        
        # å¦‚æœé‚„éœ€è¦æ›´å¤šæ¦‚å¿µï¼Œæª¢æŸ¥ä¸­å„ªå…ˆç´š
        if len(concepts) < max_concepts:
            for concept, translation in medium_priority.items():
                if concept in text and len(concepts) < max_concepts:
                    concepts.append(translation)
                    text = text.replace(concept, "")
        
        # å¦‚æœæ¦‚å¿µä¸è¶³ï¼Œå˜—è©¦æå–æ•¸å­—æˆ–ç‰¹æ®Šæ¨™è­˜
        if len(concepts) < max_concepts:
            # æå–ç‰ˆæœ¬è™Ÿã€é»æ•¸ç­‰
            import re
            version_match = re.search(r'(\d+\.?\d*)', text)
            if version_match:
                version = version_match.group(1).replace('.', '-')
                concepts.append(f"v{version}")
        
        return concepts[:max_concepts]
    
    def _clean_and_simplify_url(self, url):
        """æ¸…ç†ä¸¦ç°¡åŒ–URL"""
        # åŸºæœ¬æ¸…ç†
        url = url.lower()
        url = re.sub(r'[^\w\s-]', '', url)
        url = re.sub(r'\s+', '-', url.strip())
        url = re.sub(r'-+', '-', url)
        url = url.strip('-')
        
        # ç°¡åŒ–ï¼šé™åˆ¶æœ€å¤§é•·åº¦ç‚º50å­—ç¬¦
        max_length = 50
        if len(url) > max_length:
            parts = url.split('-')
            
            # ä¿ç•™æ—¥æœŸï¼ˆå¿…é ˆï¼‰
            essential_parts = [parts[0]]  # æ—¥æœŸ
            remaining_length = max_length - len(parts[0]) - 1
            
            # æŒ‰é‡è¦æ€§æ·»åŠ å…¶ä»–éƒ¨åˆ†
            for part in parts[1:]:
                if len(part) + 1 <= remaining_length:  # +1 for separator
                    essential_parts.append(part)
                    remaining_length -= len(part) + 1
                else:
                    break
            
            url = '-'.join(essential_parts)
        
        return url

# ç‚ºäº†å…¼å®¹æ€§ï¼Œä¹Ÿå‰µå»ºåŸå§‹é¡å
WordProcessor = ImprovedWordProcessor
WORD_PROCESSOR_EOF
          fi
          
          # æª¢æŸ¥å…¶ä»–æ¨¡çµ„
          if [ ! -f "utils.py" ] && [ ! -f "scripts/utils.py" ]; then
            cat > utils.py << 'UTILS_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import json
from pathlib import Path
from datetime import datetime

def parse_filename(filename):
    """è§£ææª”åæå–è³‡è¨Š"""
    # ç§»é™¤å‰¯æª”å
    name = Path(filename).stem
    
    # å˜—è©¦è§£ææ—¥æœŸæ ¼å¼ YYYY-MM-DD
    date_pattern = r'(\d{4}-\d{2}-\d{2})'
    date_match = re.search(date_pattern, name)
    
    is_series = False
    series_name = ""
    episode = 0
    
    if date_match:
        date_str = date_match.group(1)
        # ç§»é™¤æ—¥æœŸéƒ¨åˆ†å¾—åˆ°æ¨™é¡Œ
        title = re.sub(date_pattern + r'[-_]*', '', name).strip('-_')
    else:
        date_str = datetime.now().strftime('%Y-%m-%d')
        title = name
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºç³»åˆ—æ–‡ç«  (ep1, ep2, etc.)
    series_pattern = r'(.+?)ep(\d+)'
    series_match = re.search(series_pattern, title, re.IGNORECASE)
    
    if series_match:
        is_series = True
        series_name = series_match.group(1).strip('-_')
        episode = int(series_match.group(2))
        title = f"{series_name} ep{episode}"
    
    # æ¸…ç†æ¨™é¡Œ
    title = re.sub(r'[-_]+', ' ', title).strip()
    if not title:
        title = "æœªå‘½åæ–‡æª”"
    
    result = {
        "date": date_str,
        "title": title,
        "category": "default"
    }
    
    if is_series:
        result.update({
            "is_series": True,
            "series_name": series_name,
            "episode": episode
        })
    
    return result

def read_json(filepath, default=None):
    """è®€å–JSONæ–‡ä»¶"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return default or {}

def write_json(filepath, data):
    """å¯«å…¥JSONæ–‡ä»¶"""
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def setup_logging(level):
    """è¨­ç½®æ—¥èªŒ"""
    pass

def ensure_directories(dirs):
    """ç¢ºä¿ç›®éŒ„å­˜åœ¨"""
    for name, path in dirs.items():
        Path(path).mkdir(parents=True, exist_ok=True)
UTILS_EOF
          fi
          
          if [ ! -f "translator.py" ] && [ ! -f "scripts/translator.py" ]; then
            cat > translator.py << 'TRANSLATOR_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
from utils import read_json, write_json

class DummyTranslator:
    """ç¿»è­¯å™¨é¡"""
    
    def __init__(self, translation_dict_file=None):
        self.translation_dict_file = translation_dict_file
        self.translation_dict = self._load_translation_dict()
    
    def _load_translation_dict(self):
        """è¼‰å…¥ç¿»è­¯å­—å…¸"""
        if self.translation_dict_file:
            return read_json(self.translation_dict_file, default={})
        return {}
    
    def translate(self, text, clean_url=True):
        """ç¿»è­¯æ–‡æœ¬"""
        if not text:
            return ""
        
        # å˜—è©¦å¾å­—å…¸ç¿»è­¯
        if text in self.translation_dict:
            result = self.translation_dict[text]
        else:
            # ç°¡å–®çš„éŸ³è­¯è™•ç†
            result = text.lower()
            result = result.replace(" ", "-")
            result = result.replace("_", "-")
            
            if clean_url:
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                result = re.sub(r'[^\w\-]', '', result)
                result = re.sub(r'-+', '-', result)
                result = result.strip('-')
        
        return result

def get_translator(translation_dict_file=None, api_key=None):
    """ç²å–ç¿»è­¯å™¨å¯¦ä¾‹"""
    return DummyTranslator(translation_dict_file)
TRANSLATOR_EOF
          fi

      - name: æª¢æŸ¥Wordæ–‡æª”
        id: check_word_docs
        run: |
          if [ -d "word-docs" ]; then
            DOCX_COUNT=$(find word-docs -maxdepth 1 -name "*.docx" ! -name "~*" 2>/dev/null | wc -l)
            echo "ç™¼ç¾ $DOCX_COUNT å€‹Wordæ–‡æª”"
            
            if [ "$DOCX_COUNT" -gt 0 ]; then
              echo "has_word_docs=true" >> $GITHUB_OUTPUT
              echo "æ–‡æª”åˆ—è¡¨:"
              find word-docs -maxdepth 1 -name "*.docx" ! -name "~*" 2>/dev/null | head -5
            else
              echo "has_word_docs=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "has_word_docs=false" >> $GITHUB_OUTPUT
            echo "word-docsç›®éŒ„ä¸å­˜åœ¨"
          fi

      - name: è™•ç†Wordæ–‡æª”
        id: process_word
        if: github.event.inputs.process_word == 'true' || steps.check_word_docs.outputs.has_word_docs == 'true'
        run: |
          echo "ä½¿ç”¨åŸæœ‰åŠŸèƒ½è™•ç†Wordæ–‡æª”..."
          
          export PYTHONPATH="$(pwd):$PYTHONPATH"
          
          # æª¢æŸ¥æ˜¯å¦æœ‰åŸæœ‰çš„main.pyï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ç•¶å‰çš„æ¨¡çµ„
          if [ -f "main.py" ]; then
            echo "ä½¿ç”¨åŸæœ‰çš„main.py"
            python main.py --word-dir word-docs --output-dir blog --assets-dir assets --debug --process-all
          else
            echo "ä½¿ç”¨é‡å»ºçš„æ¨¡çµ„åŸ·è¡Œè™•ç†"
            python3 << 'MAIN_PROCESS_EOF'
import sys
import os
from pathlib import Path

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, os.getcwd())

try:
    from word_processor import ImprovedWordProcessor
    from html_generator import ImprovedHtmlGenerator
    from content_manager import ContentManager
    
    # åˆå§‹åŒ–è™•ç†å™¨ - ä½¿ç”¨åŸæœ‰çš„é¡åå’Œæ¥å£
    word_processor = ImprovedWordProcessor(
        word_dir="word-docs",
        translation_dict_file="assets/data/translation_dict.json"
    )
    
    html_generator = ImprovedHtmlGenerator(
        output_dir="blog",
        assets_dir="assets"
    )
    
    content_manager = ContentManager(
        data_dir="assets/data"
    )
    
    # æƒææ–‡æª” - ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
    documents = word_processor.scan_documents(process_all=True)
    print(f"æ‰¾åˆ° {len(documents)} å€‹æ–‡æª”éœ€è¦è™•ç†")
    
    success_count = 0
    
    for doc_path in documents:
        print(f"è™•ç†æ–‡æª”: {doc_path}")
        
        try:
            # æ­¥é©Ÿ1: æº–å‚™æ–‡æª” - ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
            doc_info = word_processor.prepare_document(doc_path)
            
            if not doc_info.get("prepared", False):
                print(f"æ–‡æª”æº–å‚™å¤±æ•—: {doc_info.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                continue
            
            # æ­¥é©Ÿ2: è™•ç†æ–‡ç« ä¿¡æ¯
            doc_info, category, tags = content_manager.process_article(doc_info)
            
            # æ­¥é©Ÿ3: ç”ŸæˆHTML - ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
            html, output_file = html_generator.generate_html(
                doc_info, category, tags, word_processor.translator
            )
            
            if output_file and output_file.exists():
                print(f"âœ“ HTMLç”ŸæˆæˆåŠŸ: {output_file}")
                
                # æ›´æ–°åšå®¢è³‡æ–™åº«
                content_manager.update_blog_post(doc_info)
                
                # æ­¥é©Ÿ4: å®Œæˆè™•ç† - ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
                word_processor.finalize_document_processing(doc_info, success=True)
                success_count += 1
            else:
                print(f"âœ— HTMLç”Ÿæˆå¤±æ•—")
                
        except Exception as e:
            print(f"è™•ç†æ–‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    print(f"è™•ç†å®Œæˆ: æˆåŠŸ {success_count} å€‹æ–‡æª”")
    
    # è¨­ç½®è¼¸å‡ºè®Šæ•¸
    if success_count > 0:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=true\n')
    else:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=false\n')

except ImportError as e:
    print(f"æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
    print("å›é€€åˆ°ç°¡åŒ–è™•ç†...")
    # é€™è£¡å¯ä»¥æ·»åŠ ç°¡åŒ–çš„è™•ç†é‚è¼¯
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        f.write('html_changed=false\n')
MAIN_PROCESS_EOF
          fi

      - name: ä¸Šå‚³ä¸¦æäº¤è½‰æ›å¾Œçš„ HTML
        if: steps.process_word.outputs.html_changed == 'true'
        run: |
          git add blog/*.html || true
          git add assets/data/*.json || true
          git add word-docs/processed/*.docx || true
          
          if ! git diff --cached --quiet; then
            CHANGED_FILES=$(git diff --name-only --cached | wc -l)
            git commit -m "è‡ªå‹•è™•ç†Wordæ–‡æª”: æ›´æ–° $CHANGED_FILES å€‹æ–‡ä»¶ [$(date '+%Y-%m-%d %H:%M:%S')]"
            git push
            echo "âœ… å·²æäº¤ $CHANGED_FILES å€‹æ–‡ä»¶è®Šæ›´"
          fi

      - name: åŸ·è¡Œçµæœå ±å‘Š
        run: |
          echo "================================================"
          echo "            è‡ªå‹•åŒ–è™•ç†åŸ·è¡Œå®Œæˆ"
          echo "================================================"
          echo ""
          echo "ğŸ• åŸ·è¡Œæ™‚é–“: $(date '+%Y-%m-%d %H:%M:%S UTC')"
          echo "ğŸŒ å°ç£æ™‚é–“: $(TZ='Asia/Taipei' date '+%Y-%m-%d %H:%M:%S %Z')"
          echo ""
          
          # æ–‡ä»¶çµ±è¨ˆ
          echo "ğŸ“Š æ–‡ä»¶çµ±è¨ˆ:"
          HTML_COUNT=$(find blog -name "*.html" 2>/dev/null | wc -l)
          JSON_COUNT=$(find assets/data -name "*.json" 2>/dev/null | wc -l)
          DOCX_COUNT=$(find word-docs -name "*.docx" 2>/dev/null | wc -l)
          PROCESSED_COUNT=$(find word-docs/processed -name "*.docx" 2>/dev/null | wc -l)
          
          echo "  ğŸ“„ HTMLæ–‡ä»¶: $HTML_COUNT å€‹"
          echo "  ğŸ“Š JSONæ–‡ä»¶: $JSON_COUNT å€‹"
          echo "  ğŸ“ Wordæ–‡æª”: $DOCX_COUNT å€‹"
          echo "  âœ… å·²è™•ç†: $PROCESSED_COUNT å€‹"
          
          # é¡¯ç¤ºæœ€æ–°æ–‡ä»¶
          echo ""
          echo "ğŸ“‹ æœ€æ–°ç”Ÿæˆçš„HTMLæ–‡ä»¶:"
          if [ "$HTML_COUNT" -gt 0 ]; then
            find blog -name "*.html" -printf "%f\n" 2>/dev/null | sort -r | head -5 | sed 's/^/  ğŸ“ /'
          else
            echo "  ï¼ˆç„¡HTMLæ–‡ä»¶ï¼‰"
          fi
          
          echo ""
          echo "ğŸ‰ åŸæœ‰åŠŸèƒ½è™•ç†æµç¨‹åŸ·è¡Œå®Œæˆï¼"
          echo "ğŸŒ ç¶²ç«™åœ°å€: https://www.horgoscpa.com"