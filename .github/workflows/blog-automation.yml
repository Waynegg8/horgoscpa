name: 部落格與影片自動化處理

on:
  workflow_dispatch:
    inputs:
      process_word:
        description: '處理Word文檔'
        required: true
        default: 'true'
        type: boolean
      update_json:
        description: '更新JSON文件'
        required: true
        default: 'true'
        type: boolean
      handle_deletion:
        description: '處理文章刪除'
        required: true
        default: 'true'
        type: boolean
      update_videos:
        description: '更新影片數據'
        required: true
        default: 'true'
        type: boolean
      force_scan:
        description: '強制掃描所有HTML文章'
        required: true
        default: 'false'
        type: boolean
      update_sitemap:
        description: '更新網站Sitemap'
        required: true
        default: 'true'
        type: boolean
      publish_scheduled:
        description: '發布排程文章'
        required: true
        default: 'true'
        type: boolean
      update_dictionary:
        description: '更新翻譯字典'
        required: true
        default: 'true'
        type: boolean
      deploy_ai_chat:
        description: '部署AI客服功能'
        required: true
        default: 'true'
        type: boolean
  
  push:
    paths:
      - '**/*'
  
  delete:
    paths:
      - 'blog/**/*.html'
      - 'services/**/*.html'
      - '*.html'
  
  schedule:
    - cron: '0 1 * * *'
    
  repository_dispatch:
    types: [delete-article, upload-article, scan-articles, update-video, update-sitemap, publish-scheduled, update-dictionary, deploy-ai-chat]

jobs:
  content-automation:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: write
    
    steps:
      - name: 檢出代碼
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: 設置 Git 配置
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
      
      - name: 設置 Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: 安裝依賴套件
        run: |
          python -m pip install --upgrade pip
          pip install loguru python-docx beautifulsoup4 requests pathlib
      
      - name: 創建必要的目錄
        run: |
          mkdir -p blog
          mkdir -p assets/data
          mkdir -p assets/images/blog
          mkdir -p assets/css
          mkdir -p assets/js
          mkdir -p word-docs/processed
          mkdir -p video
          mkdir -p .github/scripts
          mkdir -p logs
          mkdir -p scripts

      - name: 重建原有模組結構
        run: |
          # 檢查並創建原有的模組文件
          echo "重建原有的完整模組結構..."
          
          # 如果原有文件存在，使用原有文件；否則重建
          if [ ! -f "word_processor.py" ] && [ ! -f "scripts/word_processor.py" ]; then
            cat > word_processor.py << 'WORD_PROCESSOR_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from loguru import logger
import docx
from docx.opc.exceptions import PackageNotFoundError

from utils import parse_filename, read_json, write_json
from translator import get_translator

class ImprovedWordProcessor:
    """改進版 Word 文檔處理類"""
    
    def __init__(self, word_dir="word-docs", processed_dir=None, translation_dict_file=None, api_key=None):
        self.word_dir = Path(word_dir)
        self.processed_dir = Path(processed_dir or os.path.join(word_dir, "processed"))
        self.translation_dict_file = Path(translation_dict_file or "assets/data/translation_dict.json")
        self.processed_files_file = Path("assets/data/processed_files.json")
        
        # 確保目錄存在
        self.word_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.translation_dict_file.parent.mkdir(parents=True, exist_ok=True)
        
        # 初始化翻譯器
        self.translator = get_translator(self.translation_dict_file, api_key)
        
        # 載入已處理文件記錄
        self.processed_files = read_json(self.processed_files_file, default={"files": []})
        if "files" not in self.processed_files:
            self.processed_files["files"] = []
        
        # 載入關鍵詞映射表
        self.keyword_mappings = self._load_keyword_mappings()
    
    def _load_keyword_mappings(self):
        """載入中英文關鍵詞映射表"""
        mappings_file = Path("assets/data/keyword_mappings.json")
        
        # 預設的關鍵詞映射
        default_mappings = {
            # 稅務相關
            "稅務": "tax", "稅法": "tax-law", "稅務規劃": "tax-planning",
            "節稅": "tax-saving", "避稅": "tax-avoidance", "稅務申報": "tax-filing",
            "遺產稅": "estate-tax", "贈與稅": "gift-tax", "所得稅": "income-tax",
            "營業稅": "business-tax", "房屋稅": "house-tax", "地價稅": "land-value-tax",
            "印花稅": "stamp-tax",
            
            # 保險相關
            "保險": "insurance", "人壽保險": "life-insurance", "保險給付": "insurance-benefit",
            "保險規劃": "insurance-planning", "保單": "insurance-policy", "受益人": "beneficiary",
            "要保人": "policyholder", "被保險人": "insured",
            
            # 會計相關
            "會計": "accounting", "帳務": "bookkeeping", "財務": "finance",
            "審計": "audit", "財報": "financial-report", "成本": "cost",
            "費用": "expense", "收入": "income", "資產": "asset", "負債": "liability",
            
            # 企業相關
            "企業": "business", "公司": "company", "工商登記": "business-registration",
            "營運": "operation", "投資": "investment", "理財": "financial-planning",
        }
        
        # 如果映射文件存在，載入並合併
        if mappings_file.exists():
            try:
                custom_mappings = read_json(mappings_file)
                default_mappings.update(custom_mappings)
            except Exception as e:
                logger.warning(f"載入自定義關鍵詞映射失敗: {e}，使用預設映射")
        else:
            # 保存預設映射到文件
            write_json(mappings_file, default_mappings)
        
        return default_mappings
    
    def scan_documents(self, process_all=False, process_date=None, current_date=None):
        """掃描 Word 文檔目錄"""
        if current_date is None:
            current_date = datetime.now().date()
        
        documents = []
        skipped_current_future = []
        
        word_extensions = [".docx", ".doc"]
        
        logger.info(f"開始掃描文件夾: {self.word_dir}, 當前日期: {current_date}")
        
        for file in self.word_dir.glob("*"):
            if file.is_dir():
                continue
            
            if file.suffix.lower() not in word_extensions:
                continue
            
            if str(file) in self.processed_files["files"]:
                logger.debug(f"跳過已處理文件: {file}")
                continue
            
            file_info = parse_filename(file.name)
            if not file_info:
                logger.warning(f"無法解析檔名: {file.name}")
                continue
            
            try:
                file_date = datetime.strptime(file_info["date"], "%Y-%m-%d").date()
            except ValueError:
                logger.warning(f"檔名中的日期格式無效: {file_info['date']}")
                continue
            
            if process_all:
                documents.append(file)
            elif process_date:
                if file_date == process_date:
                    documents.append(file)
            else:
                if file_date <= current_date:
                    documents.append(file)
                else:
                    skipped_current_future.append(str(file))
        
        if skipped_current_future:
            logger.warning(f"跳過了{len(skipped_current_future)}個未來日期的文件")
        
        logger.info(f"找到{len(documents)}個需要處理的文件")
        return documents
    
    def extract_content(self, doc_path):
        """提取 Word 文檔內容"""
        doc_path = Path(doc_path)
        
        file_info = parse_filename(doc_path.name)
        if not file_info:
            raise ValueError(f"無法解析檔名: {doc_path.name}")
        
        try:
            doc = docx.Document(doc_path)
        except PackageNotFoundError:
            raise ValueError(f"無法讀取 Word 文檔: {doc_path}")
        
        title = doc.paragraphs[0].text.strip() if doc.paragraphs else file_info["title"]
        
        content = []
        for i, para in enumerate(doc.paragraphs):
            if not para.text.strip():
                continue
            
            if i == 0 and para.text.strip() == title:
                continue
            
            content.append(para.text)
        
        summary = ""
        if content:
            summary = content[0]
            if len(content) > 1:
                summary += " " + content[1]
            summary = summary[:200] + "..." if len(summary) > 200 else summary
        
        result = {
            "filename": doc_path.name,
            "original_filename": doc_path.name,
            "file_info": file_info,
            "title": title,
            "content": content,
            "summary": summary,
            "date": file_info["date"],
            "source_path": str(doc_path)
        }
        
        if file_info.get("is_series", False):
            result["is_series"] = True
            result["series_name"] = file_info["series_name"]
            result["episode"] = file_info["episode"]
        else:
            result["is_series"] = False
        
        return result
    
    def generate_meaningful_url(self, doc_info):
        """生成簡潔有意義的 SEO 友善 URL"""
        # 提取日期 - 從檔名解析的日期
        date = doc_info["date"]
        
        # 初始化 URL 組件
        url_components = [date]
        
        # 處理系列文章
        if doc_info.get("is_series", False):
            series_name = doc_info.get("series_name", "")
            if series_name:
                # 提取系列名稱的核心概念（只取最重要的一個詞）
                series_core = self._extract_core_concept(series_name)
                if series_core:
                    url_components.append(series_core)
            
            # 添加集數
            url_components.append(f"ep{doc_info['episode']}")
        else:
            # 非系列文章，提取標題的核心意涵
            title = doc_info.get("title", "")
            if title:
                # 只提取1-2個最核心的概念
                core_concepts = self._extract_core_concepts(title, max_concepts=2)
                if core_concepts:
                    url_components.extend(core_concepts)
        
        # 組合URL
        url = "-".join(url_components)
        
        # 最終清理，確保簡潔
        url = self._clean_and_simplify_url(url)
        
        return url
    
    def prepare_document(self, doc_path):
        """準備文檔處理（統一接口）"""
        try:
            doc_info = self.extract_content(doc_path)
            doc_info["url"] = self.generate_meaningful_url(doc_info)
            doc_info["prepared"] = True
            return doc_info
        except Exception as e:
            return {"prepared": False, "error": str(e)}
    
    def finalize_document_processing(self, doc_info, success=False):
        """完成文檔處理"""
        if success:
            # 移動文件到已處理目錄
            source_path = Path(doc_info["source_path"])
            if source_path.exists():
                processed_path = self.processed_dir / source_path.name
                shutil.move(str(source_path), str(processed_path))
                
                # 記錄已處理文件
                if str(source_path) not in self.processed_files["files"]:
                    self.processed_files["files"].append(str(source_path))
                    write_json(self.processed_files_file, self.processed_files)
                
                doc_info["processed"] = True
                doc_info["processed_path"] = str(processed_path)
            else:
                doc_info["processed"] = False
                doc_info["error"] = "源文件不存在"
        else:
            doc_info["processed"] = False
        
        return doc_info
    
    def _extract_meaningful_keywords(self, text):
        """從文本中提取有意義的關鍵詞"""
        keywords = []
        
        # 1. 先匹配已知的專業詞彙映射
        for zh_term, en_term in self.keyword_mappings.items():
            if zh_term in text:
                keywords.append((zh_term, en_term))
        
        # 2. 如果關鍵詞不足，使用智能分詞
        if len(keywords) < 3:
            extracted_terms = self._intelligent_term_extraction(text)
            for term in extracted_terms:
                if term not in [k[0] for k in keywords]:
                    # 對未映射的詞彙進行翻譯
                    translated = self.translator.translate(term, clean_url=True)
                    keywords.append((term, translated))
        
        # 3. 去重並限制數量
        unique_keywords = []
        seen_en = set()
        for zh, en in keywords:
            if en not in seen_en and len(unique_keywords) < 5:
                unique_keywords.append((zh, en))
                seen_en.add(en)
        
        return unique_keywords
    
    def _intelligent_term_extraction(self, text):
        """智能詞彙提取"""
        # 移除標點符號
        clean_text = re.sub(r'[^\w\s]', '', text)
        
        # 財稅專業詞彙特徵模式
        professional_patterns = [
            r'\w*稅\w*',     # 包含"稅"的詞
            r'\w*險\w*',     # 包含"險"的詞
            r'\w*務\w*',     # 包含"務"的詞
            r'\w*計\w*',     # 包含"計"的詞
            r'\w*財\w*',     # 包含"財"的詞
            r'\w*法\w*',     # 包含"法"的詞
            r'\w*規\w*',     # 包含"規"的詞
            r'\w*劃\w*',     # 包含"劃"的詞
        ]
        
        extracted_terms = []
        
        # 使用模式匹配提取專業詞彙
        for pattern in professional_patterns:
            matches = re.findall(pattern, clean_text)
            for match in matches:
                if len(match) >= 2 and len(match) <= 6:  # 合理的詞彙長度
                    extracted_terms.append(match)
        
        # 去重並排序（按長度降序，優先選擇較長的詞彙）
        unique_terms = list(set(extracted_terms))
        unique_terms.sort(key=len, reverse=True)
        
        return unique_terms[:5]
    
    def _extract_core_concept(self, text):
        """提取單一核心概念"""
        # 核心概念優先級列表（按重要性排序）
        core_concepts = {
            # 稅務核心概念
            "房地合一": "real-estate-tax",
            "遺產稅": "estate-tax",
            "贈與稅": "gift-tax", 
            "所得稅": "income-tax",
            "營業稅": "business-tax",
            "綜所稅": "income-tax",
            "稅務": "tax",
            
            # 保險核心概念
            "人壽保險": "life-insurance",
            "保險給付": "insurance-benefit",
            "保險": "insurance",
            
            # 會計核心概念
            "財務": "finance",
            "會計": "accounting",
            "審計": "audit",
            
            # 房地產核心概念
            "房地產": "real-estate",
            "不動產": "real-estate",
            "房屋": "property",
            
            # 公司相關
            "公司": "company",
            "企業": "business"
        }
        
        # 按長度排序，優先匹配較長的詞彙
        for concept in sorted(core_concepts.keys(), key=len, reverse=True):
            if concept in text:
                return core_concepts[concept]
        
        # 如果沒有匹配到核心概念，返回通用翻譯
        return self.translator.translate(text[:6], clean_url=True)  # 只取前6個字符
    
    def _extract_core_concepts(self, text, max_concepts=2):
        """提取多個核心概念，但限制數量"""
        concepts = []
        
        # 高優先級概念（這些概念更重要，優先提取）
        high_priority = {
            "房地合一": "real-estate-tax",
            "遺產稅": "estate-tax", 
            "贈與稅": "gift-tax",
            "所得稅": "income-tax",
            "營業稅": "business-tax",
            "人壽保險": "life-insurance",
            "保險給付": "insurance-benefit",
        }
        
        # 中優先級概念
        medium_priority = {
            "稅務": "tax",
            "保險": "insurance", 
            "會計": "accounting",
            "財務": "finance",
            "房地產": "real-estate",
            "公司": "company",
            "申報": "filing",
            "規劃": "planning"
        }
        
        # 先檢查高優先級概念
        for concept, translation in high_priority.items():
            if concept in text and len(concepts) < max_concepts:
                concepts.append(translation)
                text = text.replace(concept, "")  # 移除已匹配的部分
        
        # 如果還需要更多概念，檢查中優先級
        if len(concepts) < max_concepts:
            for concept, translation in medium_priority.items():
                if concept in text and len(concepts) < max_concepts:
                    concepts.append(translation)
                    text = text.replace(concept, "")
        
        # 如果概念不足，嘗試提取數字或特殊標識
        if len(concepts) < max_concepts:
            # 提取版本號、點數等
            import re
            version_match = re.search(r'(\d+\.?\d*)', text)
            if version_match:
                version = version_match.group(1).replace('.', '-')
                concepts.append(f"v{version}")
        
        return concepts[:max_concepts]
    
    def _clean_and_simplify_url(self, url):
        """清理並簡化URL"""
        # 基本清理
        url = url.lower()
        url = re.sub(r'[^\w\s-]', '', url)
        url = re.sub(r'\s+', '-', url.strip())
        url = re.sub(r'-+', '-', url)
        url = url.strip('-')
        
        # 簡化：限制最大長度為50字符
        max_length = 50
        if len(url) > max_length:
            parts = url.split('-')
            
            # 保留日期（必須）
            essential_parts = [parts[0]]  # 日期
            remaining_length = max_length - len(parts[0]) - 1
            
            # 按重要性添加其他部分
            for part in parts[1:]:
                if len(part) + 1 <= remaining_length:  # +1 for separator
                    essential_parts.append(part)
                    remaining_length -= len(part) + 1
                else:
                    break
            
            url = '-'.join(essential_parts)
        
        return url

# 為了兼容性，也創建原始類名
WordProcessor = ImprovedWordProcessor
WORD_PROCESSOR_EOF
          fi
          
          # 檢查其他模組
          if [ ! -f "utils.py" ] && [ ! -f "scripts/utils.py" ]; then
            cat > utils.py << 'UTILS_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import json
from pathlib import Path
from datetime import datetime

def parse_filename(filename):
    """解析檔名提取資訊"""
    # 移除副檔名
    name = Path(filename).stem
    
    # 嘗試解析日期格式 YYYY-MM-DD
    date_pattern = r'(\d{4}-\d{2}-\d{2})'
    date_match = re.search(date_pattern, name)
    
    is_series = False
    series_name = ""
    episode = 0
    
    if date_match:
        date_str = date_match.group(1)
        # 移除日期部分得到標題
        title = re.sub(date_pattern + r'[-_]*', '', name).strip('-_')
    else:
        date_str = datetime.now().strftime('%Y-%m-%d')
        title = name
    
    # 檢查是否為系列文章 (ep1, ep2, etc.)
    series_pattern = r'(.+?)ep(\d+)'
    series_match = re.search(series_pattern, title, re.IGNORECASE)
    
    if series_match:
        is_series = True
        series_name = series_match.group(1).strip('-_')
        episode = int(series_match.group(2))
        title = f"{series_name} ep{episode}"
    
    # 清理標題
    title = re.sub(r'[-_]+', ' ', title).strip()
    if not title:
        title = "未命名文檔"
    
    result = {
        "date": date_str,
        "title": title,
        "category": "default"
    }
    
    if is_series:
        result.update({
            "is_series": True,
            "series_name": series_name,
            "episode": episode
        })
    
    return result

def read_json(filepath, default=None):
    """讀取JSON文件"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return default or {}

def write_json(filepath, data):
    """寫入JSON文件"""
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def setup_logging(level):
    """設置日誌"""
    pass

def ensure_directories(dirs):
    """確保目錄存在"""
    for name, path in dirs.items():
        Path(path).mkdir(parents=True, exist_ok=True)
UTILS_EOF
          fi
          
          if [ ! -f "translator.py" ] && [ ! -f "scripts/translator.py" ]; then
            cat > translator.py << 'TRANSLATOR_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
from utils import read_json, write_json

class DummyTranslator:
    """翻譯器類"""
    
    def __init__(self, translation_dict_file=None):
        self.translation_dict_file = translation_dict_file
        self.translation_dict = self._load_translation_dict()
    
    def _load_translation_dict(self):
        """載入翻譯字典"""
        if self.translation_dict_file:
            return read_json(self.translation_dict_file, default={})
        return {}
    
    def translate(self, text, clean_url=True):
        """翻譯文本"""
        if not text:
            return ""
        
        # 嘗試從字典翻譯
        if text in self.translation_dict:
            result = self.translation_dict[text]
        else:
            # 簡單的音譯處理
            result = text.lower()
            result = result.replace(" ", "-")
            result = result.replace("_", "-")
            
            if clean_url:
                # 移除特殊字符
                result = re.sub(r'[^\w\-]', '', result)
                result = re.sub(r'-+', '-', result)
                result = result.strip('-')
        
        return result

def get_translator(translation_dict_file=None, api_key=None):
    """獲取翻譯器實例"""
    return DummyTranslator(translation_dict_file)
TRANSLATOR_EOF
          fi

      - name: 檢查Word文檔
        id: check_word_docs
        run: |
          if [ -d "word-docs" ]; then
            DOCX_COUNT=$(find word-docs -maxdepth 1 -name "*.docx" ! -name "~*" 2>/dev/null | wc -l)
            echo "發現 $DOCX_COUNT 個Word文檔"
            
            if [ "$DOCX_COUNT" -gt 0 ]; then
              echo "has_word_docs=true" >> $GITHUB_OUTPUT
              echo "文檔列表:"
              find word-docs -maxdepth 1 -name "*.docx" ! -name "~*" 2>/dev/null | head -5
            else
              echo "has_word_docs=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "has_word_docs=false" >> $GITHUB_OUTPUT
            echo "word-docs目錄不存在"
          fi

      - name: 處理Word文檔
        id: process_word
        if: github.event.inputs.process_word == 'true' || steps.check_word_docs.outputs.has_word_docs == 'true'
        run: |
          echo "使用原有功能處理Word文檔..."
          
          export PYTHONPATH="$(pwd):$PYTHONPATH"
          
          # 檢查是否有原有的main.py，如果沒有則使用當前的模組
          if [ -f "main.py" ]; then
            echo "使用原有的main.py"
            python main.py --word-dir word-docs --output-dir blog --assets-dir assets --debug --process-all
          else
            echo "使用重建的模組執行處理"
            python3 << 'MAIN_PROCESS_EOF'
import sys
import os
from pathlib import Path

# 添加當前目錄到Python路徑
sys.path.insert(0, os.getcwd())

try:
    from word_processor import ImprovedWordProcessor
    from html_generator import ImprovedHtmlGenerator
    from content_manager import ContentManager
    
    # 初始化處理器 - 使用原有的類名和接口
    word_processor = ImprovedWordProcessor(
        word_dir="word-docs",
        translation_dict_file="assets/data/translation_dict.json"
    )
    
    html_generator = ImprovedHtmlGenerator(
        output_dir="blog",
        assets_dir="assets"
    )
    
    content_manager = ContentManager(
        data_dir="assets/data"
    )
    
    # 掃描文檔 - 使用原有的方法
    documents = word_processor.scan_documents(process_all=True)
    print(f"找到 {len(documents)} 個文檔需要處理")
    
    success_count = 0
    
    for doc_path in documents:
        print(f"處理文檔: {doc_path}")
        
        try:
            # 步驟1: 準備文檔 - 使用原有的方法
            doc_info = word_processor.prepare_document(doc_path)
            
            if not doc_info.get("prepared", False):
                print(f"文檔準備失敗: {doc_info.get('error', '未知錯誤')}")
                continue
            
            # 步驟2: 處理文章信息
            doc_info, category, tags = content_manager.process_article(doc_info)
            
            # 步驟3: 生成HTML - 使用原有的方法
            html, output_file = html_generator.generate_html(
                doc_info, category, tags, word_processor.translator
            )
            
            if output_file and output_file.exists():
                print(f"✓ HTML生成成功: {output_file}")
                
                # 更新博客資料庫
                content_manager.update_blog_post(doc_info)
                
                # 步驟4: 完成處理 - 使用原有的方法
                word_processor.finalize_document_processing(doc_info, success=True)
                success_count += 1
            else:
                print(f"✗ HTML生成失敗")
                
        except Exception as e:
            print(f"處理文檔時發生錯誤: {e}")
            continue
    
    print(f"處理完成: 成功 {success_count} 個文檔")
    
    # 設置輸出變數
    if success_count > 0:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=true\n')
    else:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=false\n')

except ImportError as e:
    print(f"模組導入失敗: {e}")
    print("回退到簡化處理...")
    # 這裡可以添加簡化的處理邏輯
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        f.write('html_changed=false\n')
MAIN_PROCESS_EOF
          fi

      - name: 上傳並提交轉換後的 HTML
        if: steps.process_word.outputs.html_changed == 'true'
        run: |
          git add blog/*.html || true
          git add assets/data/*.json || true
          git add word-docs/processed/*.docx || true
          
          if ! git diff --cached --quiet; then
            CHANGED_FILES=$(git diff --name-only --cached | wc -l)
            git commit -m "自動處理Word文檔: 更新 $CHANGED_FILES 個文件 [$(date '+%Y-%m-%d %H:%M:%S')]"
            git push
            echo "✅ 已提交 $CHANGED_FILES 個文件變更"
          fi

      - name: 執行結果報告
        run: |
          echo "================================================"
          echo "            自動化處理執行完成"
          echo "================================================"
          echo ""
          echo "🕐 執行時間: $(date '+%Y-%m-%d %H:%M:%S UTC')"
          echo "🌏 台灣時間: $(TZ='Asia/Taipei' date '+%Y-%m-%d %H:%M:%S %Z')"
          echo ""
          
          # 文件統計
          echo "📊 文件統計:"
          HTML_COUNT=$(find blog -name "*.html" 2>/dev/null | wc -l)
          JSON_COUNT=$(find assets/data -name "*.json" 2>/dev/null | wc -l)
          DOCX_COUNT=$(find word-docs -name "*.docx" 2>/dev/null | wc -l)
          PROCESSED_COUNT=$(find word-docs/processed -name "*.docx" 2>/dev/null | wc -l)
          
          echo "  📄 HTML文件: $HTML_COUNT 個"
          echo "  📊 JSON文件: $JSON_COUNT 個"
          echo "  📝 Word文檔: $DOCX_COUNT 個"
          echo "  ✅ 已處理: $PROCESSED_COUNT 個"
          
          # 顯示最新文件
          echo ""
          echo "📋 最新生成的HTML文件:"
          if [ "$HTML_COUNT" -gt 0 ]; then
            find blog -name "*.html" -printf "%f\n" 2>/dev/null | sort -r | head -5 | sed 's/^/  📝 /'
          else
            echo "  （無HTML文件）"
          fi
          
          echo ""
          echo "🎉 原有功能處理流程執行完成！"
          echo "🌐 網站地址: https://www.horgoscpa.com"