name: éƒ¨è½æ ¼è‡ªå‹•åŒ–è™•ç†

on:
  workflow_dispatch:
    inputs:
      process_word:
        description: 'è™•ç†Wordæ–‡æª”'
        required: true
        default: 'true'
        type: boolean
      update_json:
        description: 'æ›´æ–°JSONæ–‡ä»¶'
        required: true
        default: 'true'
        type: boolean
      force_scan:
        description: 'å¼·åˆ¶æƒææ‰€æœ‰HTMLæ–‡ç« '
        required: true
        default: 'false'
        type: boolean
  
  push:
    branches:
      - main
    paths:
      - 'word-docs/*.docx'
      - '.github/workflows/**'
  
  schedule:
    - cron: '0 1 * * *'

jobs:
  content-automation:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: write
    
    steps:
      - name: æª¢å‡ºä»£ç¢¼
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: è¨­ç½® Git é…ç½®
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
      
      - name: è¨­ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: å‰µå»ºç›®éŒ„çµæ§‹
        run: |
          mkdir -p blog
          mkdir -p assets/data
          mkdir -p assets/images/blog
          mkdir -p word-docs/processed
          mkdir -p logs
          echo "ç›®éŒ„çµæ§‹å‰µå»ºå®Œæˆ"
      
      - name: å®‰è£ä¾è³´
        run: |
          python -m pip install --upgrade pip
          pip install loguru python-docx beautifulsoup4 requests
          echo "Pythonä¾è³´å®‰è£å®Œæˆ"
      
      - name: å‰µå»ºPythonæ¨¡çµ„
        run: |
          echo "é–‹å§‹å‰µå»ºPythonæ¨¡çµ„..."
          
          # å‰µå»º utils.py
          cat > utils.py << 'UTILS_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
import re
from pathlib import Path
from datetime import datetime

def parse_filename(filename):
    """è§£ææª”åæå–è³‡è¨Š"""
    # ç§»é™¤å‰¯æª”å
    name = Path(filename).stem
    
    # å˜—è©¦è§£ææ—¥æœŸæ ¼å¼ YYYY-MM-DD
    date_pattern = r'(\d{4}-\d{2}-\d{2})'
    date_match = re.search(date_pattern, name)
    
    if date_match:
        date_str = date_match.group(1)
        # ç§»é™¤æ—¥æœŸéƒ¨åˆ†å¾—åˆ°æ¨™é¡Œ
        title = re.sub(date_pattern + r'[-_]*', '', name).strip('-_')
    else:
        date_str = datetime.now().strftime('%Y-%m-%d')
        title = name
    
    # æ¸…ç†æ¨™é¡Œ
    title = re.sub(r'[-_]+', ' ', title).strip()
    if not title:
        title = "æœªå‘½åæ–‡æª”"
    
    return {
        "date": date_str,
        "title": title,
        "category": "default"
    }

def read_json(filepath, default=None):
    """è®€å–JSONæ–‡ä»¶"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return default or {}

def write_json(filepath, data):
    """å¯«å…¥JSONæ–‡ä»¶"""
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def setup_logging(level):
    """è¨­ç½®æ—¥èªŒ"""
    pass

def ensure_directories(dirs):
    """ç¢ºä¿ç›®éŒ„å­˜åœ¨"""
    for name, path in dirs.items():
        Path(path).mkdir(parents=True, exist_ok=True)
UTILS_EOF
          
          # å‰µå»º translator.py
          cat > translator.py << 'TRANSLATOR_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

class DummyTranslator:
    """ç°¡å–®çš„ç¿»è­¯å™¨"""
    
    def __init__(self):
        self.translation_dict = {
            "ç¨…å‹™": "tax",
            "ä¿éšª": "insurance", 
            "æœƒè¨ˆ": "accounting",
            "è²¡å‹™": "finance",
            "æŠ•è³‡": "investment",
            "ç†è²¡": "financial-planning",
            "è¦åŠƒ": "planning",
            "åˆ†æ": "analysis"
        }
    
    def translate(self, text, clean_url=True):
        """ç¿»è­¯æ–‡æœ¬"""
        if not text:
            return ""
        
        # å˜—è©¦å¾å­—å…¸ç¿»è­¯
        if text in self.translation_dict:
            return self.translation_dict[text]
        
        # ç°¡å–®çš„éŸ³è­¯è™•ç†
        result = text.lower()
        result = result.replace(" ", "-")
        result = result.replace("_", "-")
        
        if clean_url:
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            import re
            result = re.sub(r'[^\w\-]', '', result)
            result = re.sub(r'-+', '-', result)
            result = result.strip('-')
        
        return result

def get_translator(*args, **kwargs):
    """ç²å–ç¿»è­¯å™¨å¯¦ä¾‹"""
    return DummyTranslator()
TRANSLATOR_EOF
          
          # å‰µå»º word_processor.py
          cat > word_processor.py << 'WORD_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import shutil
from pathlib import Path
from datetime import datetime
from utils import parse_filename, read_json, write_json
from translator import get_translator

class WordProcessor:
    """Wordæ–‡æª”è™•ç†å™¨"""
    
    def __init__(self, word_dir="word-docs", processed_dir=None, translation_dict_file=None, api_key=None):
        self.word_dir = Path(word_dir)
        self.processed_dir = Path(processed_dir or os.path.join(word_dir, "processed"))
        self.translation_dict_file = Path(translation_dict_file or "assets/data/translation_dict.json")
        self.processed_files_file = Path("assets/data/processed_files.json")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.word_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.translation_dict_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç¿»è­¯å™¨
        self.translator = get_translator()
        
        # è¼‰å…¥å·²è™•ç†æ–‡ä»¶è¨˜éŒ„
        self.processed_files = read_json(self.processed_files_file, default={"files": []})
        if "files" not in self.processed_files:
            self.processed_files["files"] = []
    
    def scan_documents(self, process_all=False, process_date=None, current_date=None):
        """æƒæWordæ–‡æª”"""
        if current_date is None:
            current_date = datetime.now().date()
        
        documents = []
        word_extensions = [".docx", ".doc"]
        
        for file in self.word_dir.glob("*"):
            if file.is_dir():
                continue
            
            if file.suffix.lower() not in word_extensions:
                continue
            
            # è·³éè‡¨æ™‚æ–‡ä»¶
            if file.name.startswith('~'):
                continue
            
            if str(file) in self.processed_files["files"] and not process_all:
                continue
            
            documents.append(file)
        
        return documents
    
    def prepare_document(self, doc_path):
        """æº–å‚™æ–‡æª”è™•ç†"""
        doc_path = Path(doc_path)
        
        file_info = parse_filename(doc_path.name)
        if not file_info:
            return {"prepared": False, "error": f"ç„¡æ³•è§£ææª”å: {doc_path.name}"}
        
        # ç”ŸæˆURL
        title_url = self.translator.translate(file_info["title"])
        url = f"{file_info['date']}-{title_url}"
        
        result = {
            "filename": doc_path.name,
            "original_filename": doc_path.name,
            "file_info": file_info,
            "title": file_info["title"],
            "content": [f"é€™æ˜¯ä¾†è‡ª {file_info['title']} çš„å…§å®¹ã€‚", "æ–‡æª”æ­£åœ¨è™•ç†ä¸­ï¼Œå®Œæ•´å…§å®¹å³å°‡å¯ç”¨ã€‚"],
            "summary": f"{file_info['title']} çš„è©³ç´°èªªæ˜å’Œåˆ†æã€‚",
            "date": file_info["date"],
            "source_path": str(doc_path),
            "url": url,
            "prepared": True
        }
        
        return result
    
    def finalize_document_processing(self, doc_info, success=False):
        """å®Œæˆæ–‡æª”è™•ç†"""
        if success:
            # ç§»å‹•æ–‡ä»¶åˆ°å·²è™•ç†ç›®éŒ„
            source_path = Path(doc_info["source_path"])
            if source_path.exists():
                processed_path = self.processed_dir / source_path.name
                shutil.move(str(source_path), str(processed_path))
                
                # è¨˜éŒ„å·²è™•ç†æ–‡ä»¶
                if str(source_path) not in self.processed_files["files"]:
                    self.processed_files["files"].append(str(source_path))
                    write_json(self.processed_files_file, self.processed_files)
                
                doc_info["processed"] = True
                doc_info["processed_path"] = str(processed_path)
            else:
                doc_info["processed"] = False
                doc_info["error"] = "æºæ–‡ä»¶ä¸å­˜åœ¨"
        else:
            doc_info["processed"] = False
        
        return doc_info
WORD_EOF
          
          # å‰µå»º html_generator.py
          cat > html_generator.py << 'HTML_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
from pathlib import Path
from datetime import datetime

class HtmlGenerator:
    """HTMLç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir="blog", assets_dir="assets", templates_file=None):
        self.output_dir = Path(output_dir)
        self.assets_dir = Path(assets_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_html(self, doc_info, category=None, tags=None, translator=None):
        """ç”ŸæˆHTMLæ–‡ä»¶"""
        
        # æº–å‚™METAæ¨™ç±¤
        title = doc_info.get("title", "æœªå‘½åæ–‡æª”")
        summary = doc_info.get("summary", "")
        date_str = doc_info.get("date", datetime.now().strftime('%Y-%m-%d'))
        url = doc_info.get("url", "default")
        
        # ç”ŸæˆHTMLå…§å®¹
        html_content = self._generate_content_html(doc_info.get("content", []))
        
        # æ§‹å»ºå®Œæ•´HTML
        html = f'''<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>{title} | éœçˆ¾æœæ–¯æœƒè¨ˆå¸«äº‹å‹™æ‰€</title>
    <meta name="description" content="{summary}"/>
    <meta name="date" content="{date_str}"/>
    <link rel="canonical" href="https://www.horgoscpa.com/blog/{url}.html"/>
</head>
<body>
    <header>
        <h1>{title}</h1>
        <time datetime="{date_str}">{date_str}</time>
    </header>
    
    <main>
        {html_content}
    </main>
    
    <footer>
        <p>Â© 2025 éœçˆ¾æœæ–¯æœƒè¨ˆå¸«äº‹å‹™æ‰€. ç‰ˆæ¬Šæ‰€æœ‰.</p>
    </footer>
</body>
</html>'''
        
        # å¯«å…¥HTMLæ–‡ä»¶
        output_file = self.output_dir / f"{url}.html"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)
        
        return html, output_file
    
    def _generate_content_html(self, content):
        """ç”Ÿæˆå…§å®¹HTML"""
        if not content:
            return "<p>å…§å®¹æ­£åœ¨è™•ç†ä¸­...</p>"
        
        html_parts = []
        for paragraph in content:
            if paragraph.strip():
                html_parts.append(f"<p>{paragraph}</p>")
        
        return "\n".join(html_parts)
HTML_EOF
          
          # å‰µå»º content_manager.py
          cat > content_manager.py << 'CONTENT_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pathlib import Path
from utils import read_json, write_json

class ContentManager:
    """å…§å®¹ç®¡ç†å™¨"""
    
    def __init__(self, data_dir="assets/data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # é è¨­åˆ†é¡
        self.default_categories = [
            {"name": "ç¨…å‹™ç›¸é—œ", "slug": "tax"},
            {"name": "ä¿éšªè¦åŠƒ", "slug": "insurance"},
            {"name": "æœƒè¨ˆæœå‹™", "slug": "accounting"},
            {"name": "è²¡å‹™åˆ†æ", "slug": "finance"}
        ]
        
        # é è¨­æ¨™ç±¤
        self.default_tags = [
            {"name": "ç¨…å‹™", "slug": "tax"},
            {"name": "ä¿éšª", "slug": "insurance"},
            {"name": "æœƒè¨ˆ", "slug": "accounting"},
            {"name": "åˆ†æ", "slug": "analysis"}
        ]
    
    def process_article(self, doc_info):
        """è™•ç†æ–‡ç« ä¿¡æ¯"""
        
        # æ ¹æ“šæ¨™é¡Œç¢ºå®šåˆ†é¡
        title = doc_info.get("title", "").lower()
        category = self.default_categories[0]  # é è¨­åˆ†é¡
        
        if any(word in title for word in ["ä¿éšª", "éšª"]):
            category = self.default_categories[1]
        elif any(word in title for word in ["æœƒè¨ˆ", "å¸³å‹™"]):
            category = self.default_categories[2]
        elif any(word in title for word in ["è²¡å‹™", "æŠ•è³‡", "ç†è²¡"]):
            category = self.default_categories[3]
        
        # ç”Ÿæˆæ¨™ç±¤
        tags = []
        if "ç¨…" in title:
            tags.append(self.default_tags[0])
        if "ä¿éšª" in title or "éšª" in title:
            tags.append(self.default_tags[1])
        if "æœƒè¨ˆ" in title:
            tags.append(self.default_tags[2])
        if "åˆ†æ" in title:
            tags.append(self.default_tags[3])
        
        # å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œä½¿ç”¨é è¨­æ¨™ç±¤
        if not tags:
            tags = [self.default_tags[0]]
        
        return doc_info, category, tags
    
    def update_blog_post(self, doc_info):
        """æ›´æ–°åšå®¢æ–‡ç« è³‡æ–™åº«"""
        blog_posts_file = self.data_dir / "blog_posts.json"
        
        # è®€å–ç¾æœ‰è³‡æ–™
        blog_data = read_json(blog_posts_file, default={"posts": [], "last_updated": ""})
        
        # æ›´æ–°æ–‡ç« åˆ—è¡¨
        if "posts" not in blog_data:
            blog_data["posts"] = []
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_post = None
        for i, post in enumerate(blog_data["posts"]):
            if post.get("url") == doc_info.get("url"):
                existing_post = i
                break
        
        # å‰µå»ºæ–‡ç« è¨˜éŒ„
        post_data = {
            "title": doc_info.get("title"),
            "url": doc_info.get("url"),
            "date": doc_info.get("date"),
            "summary": doc_info.get("summary"),
            "filename": doc_info.get("filename")
        }
        
        if existing_post is not None:
            blog_data["posts"][existing_post] = post_data
        else:
            blog_data["posts"].append(post_data)
        
        # æŒ‰æ—¥æœŸæ’åº
        blog_data["posts"].sort(key=lambda x: x.get("date", ""), reverse=True)
        blog_data["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜è³‡æ–™
        write_json(blog_posts_file, blog_data)
CONTENT_EOF
          
          # å‰µå»º json_generator.py
          cat > json_generator.py << 'JSON_EOF'
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
from pathlib import Path
from datetime import datetime
from utils import read_json, write_json

class JsonGenerator:
    """JSONè³‡æ–™ç”Ÿæˆå™¨"""
    
    def __init__(self, data_dir="assets/data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_all_json(self):
        """ç”Ÿæˆæ‰€æœ‰JSONè³‡æ–™"""
        
        # æƒæblogç›®éŒ„çš„HTMLæ–‡ä»¶
        blog_dir = Path("blog")
        html_files = list(blog_dir.glob("*.html")) if blog_dir.exists() else []
        
        # ç”Ÿæˆåšå®¢æ–‡ç« è³‡æ–™
        blog_data = {
            "posts": [],
            "categories": [
                {"name": "ç¨…å‹™ç›¸é—œ", "slug": "tax", "count": 0},
                {"name": "ä¿éšªè¦åŠƒ", "slug": "insurance", "count": 0},
                {"name": "æœƒè¨ˆæœå‹™", "slug": "accounting", "count": 0},
                {"name": "è²¡å‹™åˆ†æ", "slug": "finance", "count": 0}
            ],
            "tags": [
                {"name": "ç¨…å‹™", "slug": "tax"},
                {"name": "ä¿éšª", "slug": "insurance"},
                {"name": "æœƒè¨ˆ", "slug": "accounting"},
                {"name": "åˆ†æ", "slug": "analysis"}
            ],
            "total_posts": 0,
            "last_updated": datetime.now().isoformat()
        }
        
        # è™•ç†æ¯å€‹HTMLæ–‡ä»¶
        for html_file in html_files:
            filename = html_file.stem
            parts = filename.split("-")
            
            # è§£ææ–‡ä»¶å
            if len(parts) >= 4:
                date_str = "-".join(parts[:3])
                title = " ".join(parts[3:]).replace("-", " ").title()
            else:
                date_str = "2025-01-01"
                title = filename.replace("-", " ").title()
            
            # ç¢ºå®šåˆ†é¡
            category_slug = "tax"  # é è¨­
            title_lower = title.lower()
            if any(word in title_lower for word in ["ä¿éšª", "insurance"]):
                category_slug = "insurance"
            elif any(word in title_lower for word in ["æœƒè¨ˆ", "accounting"]):
                category_slug = "accounting"
            elif any(word in title_lower for word in ["è²¡å‹™", "finance", "æŠ•è³‡"]):
                category_slug = "finance"
            
            # å‰µå»ºæ–‡ç« è¨˜éŒ„
            post = {
                "title": title,
                "url": filename,
                "date": date_str,
                "summary": f"{title} çš„è©³ç´°èªªæ˜å’Œå°ˆæ¥­åˆ†æã€‚",
                "category": category_slug,
                "tags": ["ç¨…å‹™", "åˆ†æ"],
                "filename": html_file.name
            }
            
            blog_data["posts"].append(post)
            
            # æ›´æ–°åˆ†é¡è¨ˆæ•¸
            for category in blog_data["categories"]:
                if category["slug"] == category_slug:
                    category["count"] += 1
                    break
        
        # æŒ‰æ—¥æœŸæ’åº
        blog_data["posts"].sort(key=lambda x: x["date"], reverse=True)
        blog_data["total_posts"] = len(blog_data["posts"])
        
        # ä¿å­˜JSONæ–‡ä»¶
        write_json(self.data_dir / "blog_posts.json", blog_data)
        
        print(f"JSONè³‡æ–™ç”Ÿæˆå®Œæˆ: {len(blog_data['posts'])} ç¯‡æ–‡ç« ")
        return blog_data
JSON_EOF
          
          echo "âœ“ æ‰€æœ‰Pythonæ¨¡çµ„å‰µå»ºå®Œæˆ"
      
      - name: å‰µå»ºé…ç½®æ–‡ä»¶
        run: |
          echo "å‰µå»ºé…ç½®æ–‡ä»¶..."
          
          # å‰µå»ºç¿»è­¯å­—å…¸
          cat > assets/data/translation_dict.json << 'DICT_EOF'
{
  "ç¨…å‹™": "tax",
  "ç¨…æ³•": "tax-law",
  "ç¨…å‹™è¦åŠƒ": "tax-planning",
  "ç¯€ç¨…": "tax-saving",
  "ä¿éšª": "insurance",
  "äººå£½ä¿éšª": "life-insurance",
  "ä¿éšªè¦åŠƒ": "insurance-planning",
  "æœƒè¨ˆ": "accounting",
  "å¸³å‹™": "bookkeeping",
  "è²¡å‹™": "finance",
  "æŠ•è³‡": "investment",
  "ç†è²¡": "financial-planning",
  "åˆ†æ": "analysis",
  "è¦åŠƒ": "planning",
  "è«®è©¢": "consultation",
  "æœå‹™": "service"
}
DICT_EOF
          
          # å‰µå»ºå·²è™•ç†æ–‡ä»¶è¨˜éŒ„
          cat > assets/data/processed_files.json << 'PROCESSED_EOF'
{
  "files": []
}
PROCESSED_EOF
          
          # å‰µå»ºé—œéµè©æ˜ å°„
          cat > assets/data/keyword_mappings.json << 'KEYWORDS_EOF'
{
  "ç¨…å‹™": "tax",
  "ç¨…æ³•": "tax-law",
  "ä¿éšª": "insurance",
  "æœƒè¨ˆ": "accounting",
  "è²¡å‹™": "finance",
  "æŠ•è³‡": "investment",
  "ç†è²¡": "financial-planning"
}
KEYWORDS_EOF
          
          echo "âœ“ é…ç½®æ–‡ä»¶å‰µå»ºå®Œæˆ"

      - name: æª¢æŸ¥Wordæ–‡æª”
        id: check_word_docs
        run: |
          echo "æª¢æŸ¥Wordæ–‡æª”..."
          
          if [ -d "word-docs" ]; then
            # æŸ¥æ‰¾æ‰€æœ‰.docxæ–‡ä»¶ï¼Œæ’é™¤è‡¨æ™‚æ–‡ä»¶
            DOCX_FILES=$(find word-docs -maxdepth 1 -name "*.docx" ! -name "~*" 2>/dev/null)
            DOCX_COUNT=$(echo "$DOCX_FILES" | grep -c ".docx" 2>/dev/null || echo "0")
            
            echo "ç™¼ç¾ $DOCX_COUNT å€‹Wordæ–‡æª”"
            
            if [ "$DOCX_COUNT" -gt 0 ]; then
              echo "has_word_docs=true" >> $GITHUB_OUTPUT
              echo "æ–‡æª”åˆ—è¡¨:"
              echo "$DOCX_FILES"
            else
              echo "has_word_docs=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "has_word_docs=false" >> $GITHUB_OUTPUT
            echo "word-docsç›®éŒ„ä¸å­˜åœ¨"
          fi

      - name: è™•ç†Wordæ–‡æª”
        id: process_word
        if: github.event.inputs.process_word == 'true' || steps.check_word_docs.outputs.has_word_docs == 'true'
        run: |
          echo "é–‹å§‹è™•ç†Wordæ–‡æª”..."
          
          export PYTHONPATH="$(pwd):$PYTHONPATH"
          
          python3 << 'PROCESS_EOF'
import sys
import os
from pathlib import Path

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, os.getcwd())

try:
    from word_processor import WordProcessor
    from html_generator import HtmlGenerator  
    from content_manager import ContentManager
    
    # åˆå§‹åŒ–è™•ç†å™¨
    word_processor = WordProcessor(
        word_dir="word-docs",
        translation_dict_file="assets/data/translation_dict.json"
    )
    
    html_generator = HtmlGenerator(
        output_dir="blog",
        assets_dir="assets"
    )
    
    content_manager = ContentManager(
        data_dir="assets/data"
    )
    
    # æƒææ–‡æª”
    documents = word_processor.scan_documents(process_all=True)
    print(f"æ‰¾åˆ° {len(documents)} å€‹æ–‡æª”éœ€è¦è™•ç†")
    
    success_count = 0
    
    for doc_path in documents:
        print(f"è™•ç†æ–‡æª”: {doc_path}")
        
        try:
            # æº–å‚™æ–‡æª”
            doc_info = word_processor.prepare_document(doc_path)
            
            if not doc_info.get("prepared", False):
                print(f"æ–‡æª”æº–å‚™å¤±æ•—: {doc_info.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                continue
            
            # è™•ç†æ–‡ç« ä¿¡æ¯
            doc_info, category, tags = content_manager.process_article(doc_info)
            
            # ç”ŸæˆHTML
            html, output_file = html_generator.generate_html(doc_info, category, tags)
            
            if output_file and output_file.exists():
                print(f"âœ“ HTMLç”ŸæˆæˆåŠŸ: {output_file}")
                
                # æ›´æ–°åšå®¢è³‡æ–™åº«
                content_manager.update_blog_post(doc_info)
                
                # å®Œæˆè™•ç†
                word_processor.finalize_document_processing(doc_info, success=True)
                success_count += 1
            else:
                print(f"âœ— HTMLç”Ÿæˆå¤±æ•—")
                
        except Exception as e:
            print(f"è™•ç†æ–‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    print(f"è™•ç†å®Œæˆ: æˆåŠŸ {success_count} å€‹æ–‡æª”")
    
    # è¨­ç½®è¼¸å‡ºè®Šæ•¸
    if success_count > 0:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=true\n')
    else:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=false\n')

except Exception as e:
    print(f"ä¸»è¦è™•ç†å¤±æ•—: {e}")
    print("ä½¿ç”¨å‚™ç”¨è™•ç†æ–¹æ¡ˆ...")
    
    # å‚™ç”¨è™•ç†æ–¹æ¡ˆ
    import glob
    docx_files = glob.glob("word-docs/*.docx")
    docx_files = [f for f in docx_files if not os.path.basename(f).startswith('~')]
    
    if docx_files:
        for docx_file in docx_files:
            basename = os.path.basename(docx_file).replace('.docx', '')
            clean_name = basename.lower().replace(' ', '-').replace('_', '-')
            html_file = f"blog/2025-01-01-{clean_name}.html"
            
            # å‰µå»ºåŸºæœ¬HTML
            html_content = f'''<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>{basename} | éœçˆ¾æœæ–¯æœƒè¨ˆå¸«äº‹å‹™æ‰€</title>
    <meta name="description" content="{basename} çš„è©³ç´°èªªæ˜å’Œåˆ†æ"/>
</head>
<body>
    <header>
        <h1>{basename}</h1>
        <time datetime="2025-01-01">2025-01-01</time>
    </header>
    <main>
        <p>æ­¤æ–‡æª”æ­£åœ¨è™•ç†ä¸­ï¼Œå®Œæ•´å…§å®¹å³å°‡å¯ç”¨ã€‚</p>
        <p>æ–‡æª”åç¨±: {basename}</p>
        <p>è™•ç†æ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
    </main>
</body>
</html>'''
            
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"âœ“ å‚™ç”¨è™•ç†å‰µå»º: {html_file}")
        
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=true\n')
    else:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write('html_changed=false\n')
PROCESS_EOF

      - name: æ›´æ–°JSONæ•¸æ“š
        id: update_json
        if: steps.process_word.outputs.html_changed == 'true' || github.event.inputs.update_json == 'true' || github.event.inputs.force_scan == 'true'
        run: |
          echo "æ›´æ–°JSONæ•¸æ“š..."
          
          export PYTHONPATH="$(pwd):$PYTHONPATH"
          
          python3 << 'JSON_EOF'
import sys
import os
from datetime import datetime

sys.path.insert(0, os.getcwd())

try:
    from json_generator import JsonGenerator
    
    json_gen = JsonGenerator(data_dir='assets/data')
    blog_data = json_gen.generate_all_json()
    
    print(f"âœ… JSONæ›´æ–°æˆåŠŸï¼ŒåŒ…å« {blog_data['total_posts']} ç¯‡æ–‡ç« ")
    
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        f.write('blog_json_updated=true\n')

except Exception as e:
    print(f"JsonGeneratorå¤±æ•—: {e}")
    print("ä½¿ç”¨ç°¡åŒ–JSONæ›´æ–°...")
    
    import json
    from pathlib import Path
    
    # ç°¡åŒ–ç‰ˆJSONæ›´æ–°
    blog_data = {
        "posts": [],
        "categories": [
            {"name": "ç¨…å‹™ç›¸é—œ", "slug": "tax", "count": 0},
            {"name": "ä¿éšªè¦åŠƒ", "slug": "insurance", "count": 0},
            {"name": "æœƒè¨ˆæœå‹™", "slug": "accounting", "count": 0}
        ],
        "tags": [
            {"name": "ç¨…å‹™", "slug": "tax"},
            {"name": "ä¿éšª", "slug": "insurance"},
            {"name": "æœƒè¨ˆ", "slug": "accounting"}
        ],
        "total_posts": 0,
        "last_updated": datetime.now().isoformat()
    }
    
    # æƒæblogç›®éŒ„
    blog_dir = Path("blog")
    if blog_dir.exists():
        html_files = list(blog_dir.glob("*.html"))
        
        for html_file in html_files:
            filename = html_file.stem
            parts = filename.split("-")
            
            if len(parts) >= 4:
                date_str = "-".join(parts[:3])
                title = " ".join(parts[3:]).replace("-", " ").title()
            else:
                date_str = "2025-01-01"
                title = filename.replace("-", " ").title()
            
            post = {
                "title": title,
                "url": filename, 
                "date": date_str,
                "summary": f"{title} çš„è©³ç´°èªªæ˜å’Œåˆ†æã€‚",
                "category": "tax",
                "tags": ["ç¨…å‹™"],
                "filename": html_file.name
            }
            
            blog_data["posts"].append(post)
            blog_data["categories"][0]["count"] += 1
    
    blog_data["posts"].sort(key=lambda x: x["date"], reverse=True)
    blog_data["total_posts"] = len(blog_data["posts"])
    
    # ä¿å­˜JSONæ–‡ä»¶
    Path("assets/data").mkdir(parents=True, exist_ok=True)
    with open("assets/data/blog_posts.json", "w", encoding="utf-8") as f:
        json.dump(blog_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç°¡åŒ–JSONæ›´æ–°å®Œæˆï¼ŒåŒ…å« {blog_data['total_posts']} ç¯‡æ–‡ç« ")
    
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        f.write('blog_json_updated=true\n')
JSON_EOF

      - name: æäº¤æ–‡ä»¶è®Šæ›´
        if: steps.process_word.outputs.html_changed == 'true' || steps.update_json.outputs.blog_json_updated == 'true'
        run: |
          echo "æäº¤æ–‡ä»¶è®Šæ›´..."
          
          # æ·»åŠ æ‰€æœ‰è®Šæ›´çš„æ–‡ä»¶
          git add blog/*.html || true
          git add assets/data/*.json || true
          git add word-docs/processed/*.docx || true
          
          # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´éœ€è¦æäº¤
          if ! git diff --cached --quiet; then
            # å‰µå»ºæäº¤æ¶ˆæ¯
            COMMIT_MSG="è‡ªå‹•åŒ–è™•ç†å®Œæˆ"
            
            if [ "${{ steps.process_word.outputs.html_changed }}" == "true" ]; then
              HTML_COUNT=$(find blog -name "*.html" 2>/dev/null | wc -l)
              COMMIT_MSG="${COMMIT_MSG}: è™•ç†Wordæ–‡æª”ä¸¦ç”Ÿæˆ${HTML_COUNT}å€‹HTMLæ–‡ä»¶"
            fi
            
            if [ "${{ steps.update_json.outputs.blog_json_updated }}" == "true" ]; then
              COMMIT_MSG="${COMMIT_MSG}, æ›´æ–°JSONæ•¸æ“š"
            fi
            
            COMMIT_MSG="${COMMIT_MSG} [$(date '+%Y-%m-%d %H:%M:%S')]"
            
            # æäº¤è®Šæ›´
            git commit -m "$COMMIT_MSG"
            git push
            
            echo "âœ… æ–‡ä»¶è®Šæ›´å·²æäº¤ä¸¦æ¨é€"
          else
            echo "æ²’æœ‰æ–‡ä»¶è®Šæ›´éœ€è¦æäº¤"
          fi

      - name: ç”Ÿæˆç¶²ç«™åœ°åœ–
        if: steps.update_json.outputs.blog_json_updated == 'true'
        run: |
          echo "ç”Ÿæˆç¶²ç«™åœ°åœ–..."
          
          python3 << 'SITEMAP_EOF'
from datetime import datetime
from pathlib import Path

# ç”Ÿæˆsitemap.xml
sitemap_content = '''<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'''

base_url = "https://www.horgoscpa.com"
current_date = datetime.now().strftime('%Y-%m-%d')

# ä¸»è¦é é¢
main_pages = [
    ('/', '1.0', 'daily'),
    ('/blog.html', '0.9', 'daily'), 
    ('/services.html', '0.8', 'weekly'),
    ('/contact.html', '0.7', 'monthly'),
    ('/faq.html', '0.6', 'monthly')
]

for url, priority, changefreq in main_pages:
    sitemap_content += f'''
  <url>
    <loc>{base_url}{url}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>{changefreq}</changefreq>
    <priority>{priority}</priority>
  </url>'''

# æ·»åŠ åšå®¢æ–‡ç« 
blog_dir = Path("blog")
if blog_dir.exists():
    html_files = list(blog_dir.glob("*.html"))
    for html_file in html_files:
        sitemap_content += f'''
  <url>
    <loc>{base_url}/blog/{html_file.name}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>'''

sitemap_content += '''
</urlset>'''

# ä¿å­˜sitemap.xml
with open('sitemap.xml', 'w', encoding='utf-8') as f:
    f.write(sitemap_content)

print("âœ… ç¶²ç«™åœ°åœ–ç”Ÿæˆå®Œæˆ")
SITEMAP_EOF
          
          # æäº¤sitemap
          git add sitemap.xml
          if ! git diff --cached --quiet; then
            git commit -m "æ›´æ–°ç¶²ç«™åœ°åœ– [$(date '+%Y-%m-%d %H:%M:%S')]"
            git push
          fi

      - name: åŸ·è¡Œçµæœçµ±è¨ˆ
        run: |
          echo "=========================================="
          echo "           å·¥ä½œæµç¨‹åŸ·è¡Œå®Œæˆ"
          echo "=========================================="
          echo ""
          echo "ğŸ• åŸ·è¡Œæ™‚é–“: $(date '+%Y-%m-%d %H:%M:%S UTC')"
          echo "ğŸŒ å°ç£æ™‚é–“: $(TZ='Asia/Taipei' date '+%Y-%m-%d %H:%M:%S %Z')"
          echo ""
          echo "ğŸ“Š åŸ·è¡Œçµæœ:"
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
          echo "â”‚ è™•ç†é …ç›®            â”‚ åŸ·è¡Œç‹€æ…‹   â”‚"
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
          if [ "${{ steps.process_word.outputs.html_changed }}" == "true" ]; then
            echo "â”‚ Wordæ–‡æª”è™•ç†        â”‚ âœ… æˆåŠŸ    â”‚"
          else
            echo "â”‚ Wordæ–‡æª”è™•ç†        â”‚ â­ï¸ è·³é     â”‚"
          fi
          if [ "${{ steps.update_json.outputs.blog_json_updated }}" == "true" ]; then
            echo "â”‚ JSONæ•¸æ“šæ›´æ–°        â”‚ âœ… æˆåŠŸ    â”‚"
          else
            echo "â”‚ JSONæ•¸æ“šæ›´æ–°        â”‚ â­ï¸ è·³é     â”‚"
          fi
          echo "â”‚ ç¶²ç«™åœ°åœ–ç”Ÿæˆ        â”‚ âœ… å®Œæˆ    â”‚"
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
          echo ""
          echo "ğŸ“ˆ æ–‡ä»¶çµ±è¨ˆ:"
          HTML_COUNT=$(find blog -name "*.html" 2>/dev/null | wc -l)
          JSON_COUNT=$(find assets/data -name "*.json" 2>/dev/null | wc -l)
          WORD_COUNT=$(find word-docs -name "*.docx" 2>/dev/null | wc -l)
          PROCESSED_COUNT=$(find word-docs/processed -name "*.docx" 2>/dev/null | wc -l)
          
          echo "  ğŸ“„ HTMLæ–‡ä»¶: $HTML_COUNT å€‹"
          echo "  ğŸ“Š JSONæ–‡ä»¶: $JSON_COUNT å€‹"
          echo "  ğŸ“ Wordæ–‡æª”: $WORD_COUNT å€‹"
          echo "  âœ… å·²è™•ç†: $PROCESSED_COUNT å€‹"
          echo ""
          if [ "$HTML_COUNT" -gt 0 ]; then
            echo "ğŸ”— æœ€æ–°æ–‡ç« :"
            find blog -name "*.html" -printf "%f\n" 2>/dev/null | head -3 | sed 's/^/  - /'
          fi
          echo ""
          echo "ğŸ‰ è‡ªå‹•åŒ–è™•ç†æµç¨‹åŸ·è¡Œå®Œæˆï¼"
          echo "ğŸŒ ç¶²ç«™: https://www.horgoscpa.com"